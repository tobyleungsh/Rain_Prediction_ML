{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10785 entries, 0 to 10784\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count  Dtype         \n",
      "---  ------             --------------  -----         \n",
      " 0   date               10785 non-null  datetime64[ns]\n",
      " 1   mean_temp          10785 non-null  object        \n",
      " 2   mean_cloud         10785 non-null  float64       \n",
      " 3   total_sun          10785 non-null  float64       \n",
      " 4   wind_speed         10785 non-null  object        \n",
      " 5   total_rainfall     10785 non-null  object        \n",
      " 6   relative_humidity  10785 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(2), object(4)\n",
      "memory usage: 589.9+ KB\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "data_list = ['mean_temp', 'mean_cloud','total_sun','wind_speed', 'total_rainfall', 'relative_humidity']\n",
    "\n",
    "#Data Cleaning\n",
    "\n",
    "df_list = []\n",
    "for st in data_list:\n",
    "    df_old = pd.read_csv(f\"/workspaces/weather/data/hk_{st}.csv\", index_col = False)\n",
    "    #Dropping all rows of incomplete data\n",
    "    df1 = df_old.drop(df_old[df_old['complete'] != \"C\"].index)\n",
    "    #Combining columns to make 'date' column with datetime type\n",
    "    df1['date'] = df1['Year'] + '-' + df1['Month'].astype('Int64').astype(str) + '-' + df1['Day'].astype('Int64').astype(str)\n",
    "    df1['date'] = pd.to_datetime(df1['date'])\n",
    "    df2 = df1.drop([\"Year\", 'Month', 'Day', 'complete'], axis = 1)\n",
    "    df_list.append(df2)\n",
    "#Combing all dataframes\n",
    "df_merged = reduce(lambda left,right: pd.merge(left,right,on=['date'], how='inner'), df_list)\n",
    "#Reordering columns\n",
    "df = df_merged[['date','mean_temp', 'mean_cloud','total_sun','wind_speed', 'total_rainfall', 'relative_humidity']]\n",
    "    \n",
    "\n",
    "df.info()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing datatypes and normalizing features\n",
    "df['mean_temp'] = df['mean_temp'].astype(float)\n",
    "df['wind_speed'] = df['wind_speed'].astype(float)\n",
    "df['relative_humidity'] = df['relative_humidity'].astype(float)\n",
    "df['total_rainfall'] = df['total_rainfall'].astype(float)\n",
    "\n",
    "df['wind_speed'] = df['wind_speed']/df['wind_speed'].mean()\n",
    "df['mean_cloud'] = df['mean_cloud']/100\n",
    "df['relative_humidity'] = df['relative_humidity']/100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rain?\n",
       "0.0    6813\n",
       "1.0    2488\n",
       "2.0     801\n",
       "3.0     683\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We want total_rainfall to be a categorial attribute to analyse the chance of rain. Let's also make a month column as they may be correlated.\n",
    "df.loc[df['total_rainfall'] == 0, 'rain?'] = 0\n",
    "df.loc[(df['total_rainfall'] > 0) & (df['total_rainfall'] < 10), 'rain?'] = 1\n",
    "df.loc[(df['total_rainfall'] >= 10) & (df['total_rainfall'] < 30),'rain?'] = 2\n",
    "df.loc[df['total_rainfall']>= 30, 'rain?'] = 3\n",
    "df['month'] = df['date'].dt.month.astype(int)\n",
    "df['rain?'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the data is highly skewed towards 0, i.e. it doesn't rain on most days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing data for models\n",
    "from sklearn.model_selection import train_test_split\n",
    "data = df.drop(['date', 'total_rainfall'],axis = 1)\n",
    "X = data.drop(['rain?'], axis = 1 )\n",
    "data_features = X.keys()\n",
    "y = data['rain?']\n",
    "X_train, X_test,y_train, y_test = train_test_split(X,y, train_size = 0.8, random_state = 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 7, 'min_samples_split': 4}\n",
      "F1 score for Decision Tree: 0.7209086694483078\n"
     ]
    }
   ],
   "source": [
    "#DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = { 'max_depth': [3,5,7,9,11], 'min_samples_split': [2,4,6,8,10]}\n",
    "tree_clf = DecisionTreeClassifier()\n",
    "\n",
    "g1 = GridSearchCV(tree_clf,parameters, cv = 5, n_jobs = -1)\n",
    "g1.fit(X_train,y_train)\n",
    "\n",
    "tree_best_param = g1.best_params_\n",
    "print(tree_best_param)\n",
    "\n",
    "tree_best_model = g1.best_estimator_\n",
    "f1_tree = f1_score(y_test,tree_best_model.predict(X_test), average = 'micro')\n",
    "print(f'F1 score for Decision Tree: {f1_tree}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 9, 'min_samples_split': 8}\n",
      "F1 Score for Random Forest:0.7338896615669912\n"
     ]
    }
   ],
   "source": [
    "#Training a Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "parameters = {'max_depth': [3,5,7,9,11], 'min_samples_split': [2,4,6,8,10]}\n",
    "rand_clf = RandomForestClassifier()\n",
    "\n",
    "g2 = GridSearchCV(rand_clf, parameters, cv = 5, n_jobs = -1)\n",
    "\n",
    "g2.fit(X_train,y_train)\n",
    "\n",
    "forest_best_param = g2.best_params_\n",
    "print(forest_best_param)\n",
    "\n",
    "forest_best_model = g2.best_estimator_\n",
    "f1_forest = f1_score(y_test,forest_best_model.predict(X_test), average = 'micro')\n",
    "print(f\"F1 Score for Random Forest:{f1_forest}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': np.int64(22)}\n",
      "F1 Score for kNN:0.7005099675475197\n"
     ]
    }
   ],
   "source": [
    "#Training kNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits = 5, shuffle  =True)\n",
    "parameters = {'n_neighbors': np.arange(2,30,1)}\n",
    "knn = KNeighborsClassifier()\n",
    "g3 = GridSearchCV(knn, parameters, cv =kf, n_jobs = -1)\n",
    "g3.fit(X_train,y_train)\n",
    "\n",
    "knn_best_param = g3.best_params_\n",
    "print(knn_best_param)\n",
    "\n",
    "knn_best_model = g3.best_estimator_\n",
    "f1_knn = f1_score(y_test,knn_best_model.predict(X_test), average = 'micro')\n",
    "print(f\"F1 Score for kNN:{f1_knn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 1, 'max_depth': 7, 'min_child_weight': 5, 'n_estimators': 100}\n",
      "F1 Score for XGBoost:0.7255447380621233\n"
     ]
    }
   ],
   "source": [
    "#Training XGboost\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits = 5, shuffle = True)\n",
    "\n",
    "parameters = {'min_child_weight': [1,3,5],'gamma': [0, 0.5, 1],'max_depth':[3,5,7,9],'n_estimators': [20,60,100]}\n",
    "xgbc = xgb.XGBClassifier(learning_rate=0.02, n_estimators=600, objective='multi:softmax')\n",
    "g4 = GridSearchCV(xgbc, parameters,cv = skf, n_jobs = -1)\n",
    "g4.fit(X_train, y_train)\n",
    "\n",
    "xgbc_best_param = g4.best_params_\n",
    "print(xgbc_best_param)\n",
    "\n",
    "xgbc_best_model = g4.best_estimator_\n",
    "f1_xgb = f1_score(y_test, xgbc_best_model.predict(X_test), average = 'micro')\n",
    "print(f\"F1 Score for XGBoost:{f1_xgb}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Feature  Permutation Importance\n",
      "4  relative_humidity                0.077840\n",
      "0          mean_temp                0.036300\n",
      "1         mean_cloud                0.031896\n",
      "5              month                0.031433\n",
      "2          total_sun                0.022485\n",
      "3         wind_speed                0.004033\n"
     ]
    }
   ],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "result = permutation_importance(forest_best_model, X_test, y_test, n_repeats=10, random_state=0, n_jobs=-1)\n",
    "perm_imp_df = pd.DataFrame({'Feature': data_features, 'Permutation Importance': result.importances_mean}).sort_values('Permutation Importance', ascending=False)\n",
    "print(perm_imp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detailed Analysis:\n",
    "\n",
    "All models have mediocre F1 scores, which is likely due to a highly imbalanced dataset, which may have affected model training and their precision.\n",
    "\n",
    "Random Forest performed the best out of all the models, as they are easier to tune and more robust against imbalanced datasets.\n",
    "\n",
    "XGBoost performed poorly due to poor hyperparameter tuning. Better tuning methods such as Bayesian Optimization could be implemented to further increase the model's F1 score.\n",
    "\n",
    "Relative humidity was the most important feature in model training, but it's permutation importance score is still very low. This suggests insufficient feature extraction and /or lack of relavent features.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
